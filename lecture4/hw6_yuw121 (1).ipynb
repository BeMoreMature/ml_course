{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skewed Classes\n",
    "\n",
    "In this exercise, we use digits data set of Scikit-learn. Run the code below read the description of the data set and show a sample. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])\n",
      "Optical Recognition of Handwritten Digits Data Set\n",
      "===================================================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 5620\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      "References\n",
      "----------\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe84a3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe20fd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "DigitsData=load_digits()\n",
    "print(DigitsData.keys()) \n",
    "print(DigitsData.DESCR) #read description of the dataset\n",
    "\n",
    "#plot one of the images in the data\n",
    "plt.gray() \n",
    "plt.matshow(DigitsData.images[1]) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to build classfiers that identify digit 9. For this purpose, do the following:\n",
    "\n",
    "1) Define the target value to be equal to 1 only for digit 9\n",
    "\n",
    "You can use: y= (DigitsData.target == 9)\n",
    "\n",
    "\n",
    "- Find how many times y is equal to True and how many times it is equal to False\n",
    "\n",
    "You can use: print(sum(y==True)); print(sum(y==False))\n",
    "\n",
    "\n",
    "COMMENT: what do you observe? Is the dataset for this classification problem balanced or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "1617\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "y=(DigitsData.target==9)\n",
    "print(sum(y==True))\n",
    "print(sum(y==False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is not balanced, because you can see the number of False is far larger than the number of True.\n",
    "So the distribution of True is concentrated on the left side of y axis, whereas the distribution of False focused on the right side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Find the accuracy of a dummy classifier that always selects the majority class. Use the DigitsData.data as features and y (defined above) as the response, then train_test_split to get training and test data.\n",
    "\n",
    "You can use:\n",
    "\n",
    "      X_train, X_test, Y_train, Y_test= train_test_split(DigitsData.data, y, random_state= 0)\n",
    "      from sklearn.dummy import DummyClassifier\n",
    "      dummy_majority=DummyClassifier(strategy='most_frequent').fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.8955555555555555\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(DigitsData.data, y, random_state= 0)\n",
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_majority=DummyClassifier(strategy='most_frequent').fit(X_train, Y_train)\n",
    "print(\"Accuracy is \",dummy_majority.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Build QDA classifier. Find accuracy, confusion matrix, precision, and recall\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24731182795698925\n",
      "0.9787234042553191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yuhao wu\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\discriminant_analysis.py:682: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "QDAmodelFitted = QuadraticDiscriminantAnalysis().fit(X_train, Y_train)\n",
    "PredictedOutput=QDAmodelFitted.predict(X_test)\n",
    "confusion=confusion_matrix(Y_test,PredictedOutput)\n",
    "print(precision_score(Y_test,PredictedOutput))\n",
    "print(recall_score(Y_test,PredictedOutput))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Build LDA classifier. Find accuracy, confusion matrix, precision, and recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8163265306122449\n",
      "0.851063829787234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yuhao wu\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "LDAmodelFitted = LinearDiscriminantAnalysis().fit(X_train, Y_train)\n",
    "PredictedOutput=LDAmodelFitted.predict(X_test)\n",
    "confusion=confusion_matrix(Y_test,PredictedOutput)\n",
    "print(precision_score(Y_test,PredictedOutput))\n",
    "print(recall_score(Y_test,PredictedOutput))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
